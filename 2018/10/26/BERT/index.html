<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="解决的问题对应NLP的任务来说，比如QA、MRC、chatbot等，简单预训练，加上复杂的针对任务的下游网络是否合理？">
<meta name="keywords" content="NLP,word vectors">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT">
<meta property="og:url" content="http://yoursite.com/2018/10/26/BERT/index.html">
<meta property="og:site_name" content="随想空间">
<meta property="og:description" content="解决的问题对应NLP的任务来说，比如QA、MRC、chatbot等，简单预训练，加上复杂的针对任务的下游网络是否合理？">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a0ca074cly1fwkjmili1jj20ou0cf0xb.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a0ca074cly1fwovdhk2w6j20pq09swhl.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a0ca074cly1fwow6rl9gvj20q1094q4n.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/a0ca074cly1fwp190t28dj20lq0ky79c.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/v2-b9d232ca7875ca6db8e739a34fa0ee74_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-70035953238890d3dce14b7e57966a03_b.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/v2-596cd8e36594741f803ef5fc623cdcf8_b.jpg">
<meta property="og:updated_time" content="2019-10-11T02:59:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT">
<meta name="twitter:description" content="解决的问题对应NLP的任务来说，比如QA、MRC、chatbot等，简单预训练，加上复杂的针对任务的下游网络是否合理？">
<meta name="twitter:image" content="http://ww1.sinaimg.cn/large/a0ca074cly1fwkjmili1jj20ou0cf0xb.jpg">
  <link rel="canonical" href="http://yoursite.com/2018/10/26/BERT/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>BERT | 随想空间</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">随想空间</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
        
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      
    
      
      
        
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      
    
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>
        
        搜索
        </a>
      </li>
    
  </ul>

    

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/26/BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="小石头">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="随想空间">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            BERT
            

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2018-10-26 16:22:00" itemprop="dateCreated datePublished" datetime="2018-10-26T16:22:00+08:00">2018-10-26</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-11 10:59:33" itemprop="dateModified" datetime="2019-10-11T10:59:33+08:00">2019-10-11</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/word-vectors/" itemprop="url" rel="index">
                    <span itemprop="name">word vectors</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            <span id="/2018/10/26/BERT/" class="post-meta-item leancloud_visitors" data-flag-title="BERT" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/10/26/BERT/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/10/26/BERT/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h1><p>对应NLP的任务来说，比如QA、MRC、chatbot等，简单预训练，加上复杂的针对任务的下游网络是否合理？<br><a id="more"></a></p>
<h1 id="why"><a href="#why" class="headerlink" title="why"></a>why</h1><p>一般针对已经存在的pre-training模型，针对不同的应用有两种策略来应用这种模型：<em>feature based</em>和<em>fine-tuning</em>。两种策略的典型方向对应于：</p>
<ol>
<li>feature based: ELMo</li>
<li>fine-tuning: OpenAI Generative Pre-trained Transformer(GPT)</li>
</ol>
<p>具体说，ELMo使用预训练的表征作为特定任务的额外特征。而GPT尽可能少地引入任务相关的参数，相反地在具体任务中进行微调。这两种策略在预训练过程中使用相同的目标函数，也就是<strong>单向的语言模型</strong>来进行学习通用语言的表征。</p>
<h2 id="feature-based"><a href="#feature-based" class="headerlink" title="feature based"></a>feature based</h2><p><a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo: Deep contextualized word representations</a></p>
<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><ul>
<li>解决的问题：<ul>
<li>表征单词的复杂特性：语法和语义</li>
<li>模型的多用型（polysemy）</li>
</ul>
</li>
<li>核心<ul>
<li>deep</li>
<li>function of all internal layers of biLM (bidirectional Language Model)<blockquote>
<p>each token is assigned a representation that is a function of the entire input sentence.</p>
</blockquote>
</li>
</ul>
</li>
<li>发现：<ul>
<li>high-level的LSTM state：语义</li>
<li>low-level：语法</li>
</ul>
</li>
<li>实验：<ul>
<li>textual entailment</li>
<li>QA</li>
<li>sentiment analysis</li>
</ul>
</li>
<li>比对模型：CoVe (Contextualized word vectors, NIPS-2017)</li>
<li>方向：semi-supervised</li>
</ul>
<h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><ul>
<li>双向语言模型<ul>
<li>$\sum_{k=1}^{N}(log p(t_k|t_1,…,t_{k-1};\Theta_x,\overrightarrow{\Theta}_{LSTM}, \Theta_s) + log p(t_k|t_{k+1},…,t_{N};\Theta_x,\overleftarrow{\Theta}_{LSTM}, \Theta_s))$</li>
<li>两个方向上的一些参数会共享</li>
</ul>
</li>
<li>核心：<ul>
<li>对于每一个token $t_k$，L层的biLM计算得到如下的表示：<ul>
<li>$R_k = {x_k^{LM},\overrightarrow{h}_{k,j}^{LM},\overleftarrow{h}^{LM}|j=1,…,L}$</li>
</ul>
</li>
<li><strong>function</strong>: <ul>
<li>$ELMo_k^{task} = E(R_k;\Theta^{task})=\gamma^{task}\sum_{j=0}^Ls_j^{task}h_{k,j}^{LM}$，其中$s^{task}$为归一化的不同层softmax权重</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="fine-tuning-based"><a href="#fine-tuning-based" class="headerlink" title="fine tuning based"></a>fine tuning based</h2><p><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Improving Language Understanding with Unsupervised Learning</a></p>
<h3 id="概览-1"><a href="#概览-1" class="headerlink" title="概览"></a>概览</h3><ul>
<li>方向：semi-supervised</li>
<li>解决的问题：利用大量无标签的数据，进行预训练，再在不同的带有标签的语言任务上进行fine-tune。</li>
<li>主要结构：Transformer</li>
<li>实验：四种NLU的任务：<ul>
<li>natural language inference</li>
<li>QA</li>
<li>semantic similarity</li>
<li>text classification</li>
</ul>
</li>
<li>对之前工作的吐槽：<ul>
<li>比如Word2vec虽然是基于无标签数据预训练得到，而且也得到了最广泛的应用，但是目前大多数是针对word-level的information transfer，这篇文章的作者目标是捕获更高层次的语义表征</li>
<li>传统的手段是通过phrase-level和sentence-level来实现</li>
<li>吐槽之前unsupervised用的LSTM，现在用的transformer，可以获取更长范围内的语言结构</li>
<li>之前有些工作会利用额外的无监督训练目标函数，作者吐槽虽然他们也用了，但是在无监督预训练中已经可以学到目标任务的相关信息了…</li>
</ul>
</li>
</ul>
<h3 id="框架-1"><a href="#框架-1" class="headerlink" title="框架"></a>框架</h3><p>主要分为两个部分：</p>
<ol>
<li>无监督的预训练 (unsupervised pre-training)</li>
<li>有监督的微调 (supervised fine-tuning)</li>
</ol>
<h4 id="无监督预训练"><a href="#无监督预训练" class="headerlink" title="无监督预训练"></a>无监督预训练</h4><ul>
<li>给定token：$U = {u_1,…,u_n}$</li>
<li>目标函数：$max L_1(U) = \sum_i logP(u_i|u_{i-k},…,u_{i-1};\Theta)$，其中$k$为context window</li>
<li>使用了多层Transformer decoder</li>
</ul>
<h4 id="有监督微调"><a href="#有监督微调" class="headerlink" title="有监督微调"></a>有监督微调</h4><p>对于具体任务而言：</p>
<ul>
<li>输入：$x^1, x^2,…,x^m$</li>
<li>标签：$y$</li>
<li>预训练模块：$M$，假设输入经过该模块的输出为最后一个transformer的$h_l^m$</li>
<li>模型输出：$P(y|x^1…x^m) = softmax(h_l^mW_y)$</li>
<li>目标函数：$max L_2(C) = \sum_{(x,y)}log P(y|x^1…x^m)$</li>
<li>加入无监督训练的目标函数一起优化：$L_3(C) = L_2(C) + \lambda * L_1(C)$</li>
</ul>
<h4 id="不同任务的修正"><a href="#不同任务的修正" class="headerlink" title="不同任务的修正"></a>不同任务的修正</h4><p><img src="http://ww1.sinaimg.cn/large/a0ca074cly1fwkjmili1jj20ou0cf0xb.jpg" alt=""></p>
<h1 id="what"><a href="#what" class="headerlink" title="what"></a>what</h1><p>上述的模型依然无法解决需要为每个特定任务定制各种各种花式的网络结构。BERT的目的便在于此。</p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm-up"></a>Warm-up</h3><ol>
<li>ELMo: extract <strong>context sensitive</strong>features from language model</li>
<li>Transfer Learning from unsupervised data: OpenAI GPT. Pre-train on LM objective, fine-tune on supervised downstream tasks.</li>
<li>Transfer Learning from supervised data: Need more labeled data.</li>
</ol>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><ul>
<li>multi-layer</li>
<li>bi-directional</li>
<li>Transformer</li>
</ul>
<p>总体来说，BERT可以认为是双向transformer结构的LM+fine-tuning结构:<br><img src="http://ww1.sinaimg.cn/large/a0ca074cly1fwovdhk2w6j20pq09swhl.jpg" alt=""></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul>
<li>目的：同时表征<strong>单个文本句子</strong>和<strong>文本句子对</strong>，比如QA中的Q和A</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/a0ca074cly1fwow6rl9gvj20q1094q4n.jpg" alt=""></p>
<ol>
<li>token embeddings：<ul>
<li>wordpiece embeddings</li>
<li>CLS for classification problem</li>
</ul>
</li>
<li>segment embeddings:<ul>
<li>[sep]区分句子</li>
<li>添加当前词所在句子的index embedding</li>
<li>单个句子，只使用第一个sentence embedding</li>
</ul>
</li>
<li>position embeddings:<ul>
<li>当前词偶在位置的index embedding</li>
</ul>
</li>
</ol>
<p>最后输入为三个embedding的简单相加。。。简单粗暴</p>
<h3 id="核心：预训练模型"><a href="#核心：预训练模型" class="headerlink" title="核心：预训练模型"></a>核心：预训练模型</h3><ul>
<li>无监督</li>
<li>真正意义上的双向</li>
<li>分解为两个任务</li>
</ul>
<h4 id="Task-1-Masked-LM"><a href="#Task-1-Masked-LM" class="headerlink" title="Task 1: Masked LM"></a>Task 1: Masked LM</h4><ul>
<li>首先想一下目前的双向模型是否是真正的双向？</li>
<li>为何目前的双向模型无法真正双向？</li>
</ul>
<p>解决方案：【Masked Language Model】随机（这里采用15%）覆盖某些token，然后预测这些覆盖的token，而不是像传统LM那样预测下一个词的分布。</p>
<ul>
<li><p>有没有问题？？？</p>
<ol>
<li>mismatch between pre-training and fine-tuning，换句话说，[MASK]在fine-tuning的时候未知。</li>
<li>训练过程中每个batch只有15%的数据用于预测，预训练模型收敛会变慢。</li>
</ol>
</li>
<li><p>解决方案：</p>
<ol>
<li>针对15%的token，dataloader分成三个部分：<ol>
<li>80%情况用[mask]覆盖：<code>my dog is hairy</code> -&gt; <code>my dog is [MASK]</code></li>
<li>10%情况用随机单词覆盖：<code>my dog is hairy</code> -&gt; <code>my dog is apple</code></li>
<li>10%情况不变：<code>my dog is hairy</code> -&gt; <code>my dog is hairy</code></li>
</ol>
</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get random data in dataloader</span></span><br><span class="line"><span class="keyword">for</span> i, token <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    prob = random.random()</span><br><span class="line">    <span class="keyword">if</span> prob &lt; <span class="number">0.15</span>:</span><br><span class="line">        prob /= <span class="number">0.15</span></span><br><span class="line">        <span class="comment"># 80% randomly change token to mask token</span></span><br><span class="line">        <span class="keyword">if</span> prob &lt; <span class="number">0.8</span>:</span><br><span class="line">            tokens[i] = self.vocab.mask_index</span><br><span class="line">        <span class="comment"># 10% randomly change token to random token</span></span><br><span class="line">        <span class="keyword">elif</span> prob &lt; <span class="number">0.9</span>:</span><br><span class="line">            tokens[i] = random.randrange(len(self.vocab))</span><br><span class="line">        <span class="comment"># 10% randomly change token to current token</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Task-2-Next-sentence-predition"><a href="#Task-2-Next-sentence-predition" class="headerlink" title="Task 2: Next sentence predition"></a>Task 2: Next sentence predition</h4><ul>
<li>目的：语言模型无法建模句子和句子之间的关系，该task就是为了解决这个问题。</li>
<li>语料构建：<ul>
<li>50%几率替换句子A的下一句话B</li>
<li>二分类标签：<code>isNext</code>和<code>NotNext</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_sent</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    t1, t2 = self.get_corpus_line(index)</span><br><span class="line">    <span class="comment"># output_text, label(isNotNext:0, isNext:1)</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> t1, t2, <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> t1, self.get_random_line(), <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="语料"><a href="#语料" class="headerlink" title="语料"></a>语料</h4><ul>
<li>BookCorpus(800M words）</li>
<li>English Wikipedia(2,500M words)：注意这里尽量使用具有长文本的语料，以此表征长的连续文本关系。</li>
</ul>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul>
<li>超参数：<ul>
<li>batch size：256</li>
<li>lr：1e-4, learning rate warm-up </li>
<li>Adam</li>
<li>L2 decay: 0.01</li>
<li>dropout: 0.1</li>
<li>activation： gelu</li>
</ul>
</li>
<li>损失函数：<ul>
<li>$loss(task1) + loss(task2)$</li>
</ul>
</li>
</ul>
<h4 id="计算量"><a href="#计算量" class="headerlink" title="计算量"></a>计算量</h4><blockquote>
<p>BERT-Base: L = 12, H = 768, A = 12, Total parameters = 110M<br>BERT-Large: L = 24, H = 1024, A = 16, Total parameters = 340M<br>其中L表示Transformer层数，H表示Transformer内部维度，A表示Heads的数量</p>
</blockquote>
<ul>
<li>Base model: 16TPU, 4days</li>
<li>large model: 64TPU, 4days</li>
<li>4 standard GPUs: 99days</li>
</ul>
<h4 id="如何泛化到不同任务"><a href="#如何泛化到不同任务" class="headerlink" title="如何泛化到不同任务"></a>如何泛化到不同任务</h4><p>这里比较了11种不同的NLP任务，具体参考论文。</p>
<p><img src="http://ww1.sinaimg.cn/large/a0ca074cly1fwp190t28dj20lq0ky79c.jpg" alt=""></p>
<h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><ol>
<li>pre-train model的影响</li>
</ol>
<p><img src="https://pic2.zhimg.com/v2-b9d232ca7875ca6db8e739a34fa0ee74_b.jpg" alt=""></p>
<ol start="2">
<li>training steps的影响</li>
</ol>
<p><img src="https://pic1.zhimg.com/v2-70035953238890d3dce14b7e57966a03_b.jpg" alt=""></p>
<ol start="3">
<li>BERT+feature-based</li>
</ol>
<p><img src="https://pic1.zhimg.com/v2-596cd8e36594741f803ef5fc623cdcf8_b.jpg" alt=""></p>
<h1 id="how"><a href="#how" class="headerlink" title="how"></a>how</h1><p>目前非官方的<a href="https://github.com/codertimo/BERT-pytorch" target="_blank" rel="noopener">pytorch实现</a>。</p>
<h2 id="核心模型代码"><a href="#核心模型代码" class="headerlink" title="核心模型代码"></a>核心模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model overview</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT model : Bidirectional Encoder Representations from Transformers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden=<span class="number">768</span>, n_layers=<span class="number">12</span>, attn_heads=<span class="number">12</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param vocab_size: vocab_size of total words</span></span><br><span class="line"><span class="string">        :param hidden: BERT model hidden size</span></span><br><span class="line"><span class="string">        :param n_layers: numbers of Transformer blocks(layers)</span></span><br><span class="line"><span class="string">        :param attn_heads: number of attention heads</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = hidden</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.attn_heads = attn_heads</span><br><span class="line">        <span class="comment"># paper noted they used 4*hidden_size for ff_network_hidden_size</span></span><br><span class="line">        self.feed_forward_hidden = hidden * <span class="number">4</span></span><br><span class="line">        <span class="comment"># embedding for BERT, sum of positional, segment, token embeddings</span></span><br><span class="line">        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)</span><br><span class="line">        <span class="comment"># multi-layers transformer blocks, deep network</span></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [TransformerBlock(hidden, attn_heads, hidden * <span class="number">4</span>, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)])</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, segment_info)</span>:</span></span><br><span class="line">        <span class="comment"># attention masking for padded token</span></span><br><span class="line">        <span class="comment"># torch.ByteTensor([batch_size, 1, seq_len, seq_len)</span></span><br><span class="line">        mask = (x &gt; <span class="number">0</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, x.size(<span class="number">1</span>), <span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># embedding the indexed sequence to sequence of vectors</span></span><br><span class="line">        x = self.embedding(x, segment_info)</span><br><span class="line">        <span class="comment"># running over multiple transformer blocks</span></span><br><span class="line">        <span class="keyword">for</span> transformer <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">            x = transformer.forward(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h1 id="目前对我们的用处"><a href="#目前对我们的用处" class="headerlink" title="目前对我们的用处"></a>目前对我们的用处</h1><ol>
<li>character-level word vectors: fasttext in chinese?</li>
<li>semantic analysis</li>
<li>entity recognition</li>
<li>relation extraction: Non-related corpus construction.</li>
<li>Chinese corpus? wait for model and fine-tune it…</li>
<li>对于生成模型是否有帮助（诸如NMT，image captioning）？</li>
<li>对于生成模型的benchmark是否有帮助？比如可以根据BERT的结果计算一个metric，然后针对不同任务都有类似的评价指标？</li>
<li>该网络是否可以更加简化？</li>
<li>和强化学习有和关联或者应用？</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" target="_blank" rel="noopener">ELMo: Deep contextualized word representations</a></li>
<li><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Improving Language Understanding with Unsupervised Learning</a></li>
<li><a href="https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/" target="_blank" rel="noopener">BERT reddit discussion</a></li>
</ol>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/NLP/" rel="tag"># NLP</a>
            
              <a href="/tags/word-vectors/" rel="tag"># word vectors</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2018/03/22/semi-supervised-learning-in-NLP/" rel="next" title="semi-supervised learning in NLP">
                  <i class="fa fa-chevron-left"></i> semi-supervised learning in NLP
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#解决的问题"><span class="nav-number">1.</span> <span class="nav-text">解决的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#why"><span class="nav-number">2.</span> <span class="nav-text">why</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#feature-based"><span class="nav-number">2.1.</span> <span class="nav-text">feature based</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概览"><span class="nav-number">2.1.1.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#框架"><span class="nav-number">2.1.2.</span> <span class="nav-text">框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fine-tuning-based"><span class="nav-number">2.2.</span> <span class="nav-text">fine tuning based</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概览-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#框架-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#无监督预训练"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">无监督预训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#有监督微调"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">有监督微调</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不同任务的修正"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">不同任务的修正</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#what"><span class="nav-number">3.</span> <span class="nav-text">what</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERT"><span class="nav-number">3.1.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Warm-up"><span class="nav-number">3.1.1.</span> <span class="nav-text">Warm-up</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Architecture"><span class="nav-number">3.1.2.</span> <span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#输入"><span class="nav-number">3.1.3.</span> <span class="nav-text">输入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核心：预训练模型"><span class="nav-number">3.1.4.</span> <span class="nav-text">核心：预训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Task-1-Masked-LM"><span class="nav-number">3.1.4.1.</span> <span class="nav-text">Task 1: Masked LM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Task-2-Next-sentence-predition"><span class="nav-number">3.1.4.2.</span> <span class="nav-text">Task 2: Next sentence predition</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验"><span class="nav-number">3.1.5.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#语料"><span class="nav-number">3.1.5.1.</span> <span class="nav-text">语料</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练"><span class="nav-number">3.1.5.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算量"><span class="nav-number">3.1.5.3.</span> <span class="nav-text">计算量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何泛化到不同任务"><span class="nav-number">3.1.5.4.</span> <span class="nav-text">如何泛化到不同任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#结果分析"><span class="nav-number">3.1.5.5.</span> <span class="nav-text">结果分析</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how"><span class="nav-number">4.</span> <span class="nav-text">how</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#核心模型代码"><span class="nav-number">4.1.</span> <span class="nav-text">核心模型代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目前对我们的用处"><span class="nav-number">5.</span> <span class="nav-text">目前对我们的用处</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">小石头</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/usakey" title="GitHub &rarr; https://github.com/usakey" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:rdxiaolei@gmail.com" title="E-Mail &rarr; mailto:rdxiaolei@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小石头</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">
      
    主题 – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.4.1
  </div>

        






  
  <script>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
            
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
            
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=Bw8rAxLf1cue3IcvXcEXknfi-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'Bw8rAxLf1cue3IcvXcEXknfi-gzGzoHsz',
            'X-LC-Key': 'nibkM8GWAjLXcRWeNqIpj5yh',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>






        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/muse.js?v=7.4.1"></script>
<script src="/js/next-boot.js?v=7.4.1"></script>



  








  <script src="/js/local-search.js?v=7.4.1"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://xiaoleixu.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function disqus_config() {
    this.page.url = "http://yoursite.com/2018/10/26/BERT/";
    this.page.identifier = "2018/10/26/BERT/";
    this.page.title = 'BERT';
  }
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://xiaoleixu.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    window.addEventListener('load', loadComments, false);
  
</script>

</body>
</html>
